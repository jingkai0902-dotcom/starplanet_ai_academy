#!/usr/bin/env python3
"""
æ–¯å¦æ˜ŸçƒçŸ¥è¯†åº“é’‰é’‰æœºå™¨äºº - Streamæ¨¡å¼
ä½¿ç”¨é’‰é’‰Stream SDKï¼Œæ— éœ€å…¬ç½‘IP
"""

import json
import logging
import re
import asyncio
from pathlib import Path

import requests
import dingtalk_stream
from dingtalk_stream import AckMessage
from dingtalk_stream.chatbot import ChatbotHandler, ChatbotMessage

# é…ç½®æ—¥å¿— - è¾“å‡ºåˆ°æ–‡ä»¶
log_file = Path(__file__).parent / "bot.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============== é…ç½®åŒºåŸŸ ==============
CONFIG = {
    "app_key": "",
    "app_secret": "",
    "kb_path": "",
    "llm_provider": "zhipu",
    "llm_api_key": "",
    "llm_base_url": "https://open.bigmodel.cn/api/paas/v4",
    "llm_model": "glm-4.7",
    "claude_api_key": "",
    "claude_base_url": "",
}

# ============== çŸ¥è¯†åº“ ==============
KB_DOCUMENTS = []

# ============== æ¶ˆæ¯å»é‡ ==============
# å­˜å‚¨å·²å¤„ç†çš„æ¶ˆæ¯IDï¼ˆæœ€å¤šä¿ç•™1000æ¡ï¼‰
PROCESSED_MESSAGES = set()
MAX_PROCESSED_MESSAGES = 1000

# ============== ç”¨æˆ·ä¼šè¯è®°å¿† ==============
# å­˜å‚¨æ¯ä¸ªç”¨æˆ·çš„å¯¹è¯ä¸Šä¸‹æ–‡ {sender_id: {"course_type": "STEM", "course_id": "1-1-02", "topic": "è‡ªåˆ¶è¡¨æƒ…åŒ…", "last_query": "...", "timestamp": ...}}
import time
USER_SESSIONS = {}
SESSION_TIMEOUT = 600  # ä¼šè¯è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œ10åˆ†é’Ÿ


def get_message_id(incoming_message) -> str:
    """ä»æ¶ˆæ¯ä¸­æå–å”¯ä¸€ID"""
    if isinstance(incoming_message, dict):
        # å°è¯•å¤šç§å¯èƒ½çš„IDå­—æ®µ
        msg_id = incoming_message.get("msgId") or incoming_message.get("conversationId") or incoming_message.get("createAt")
        if msg_id:
            return str(msg_id)
        # ç”¨å†…å®¹+å‘é€è€…+æ—¶é—´ç”Ÿæˆä¼ªID
        content = incoming_message.get("text", {})
        if isinstance(content, dict):
            content = content.get("content", "")
        sender = incoming_message.get("senderId", "")
        return f"{sender}_{hash(content)}_{incoming_message.get('createAt', '')}"
    else:
        # ChatbotMessageå¯¹è±¡
        msg_id = getattr(incoming_message, 'msg_id', None) or getattr(incoming_message, 'conversation_id', None)
        if msg_id:
            return str(msg_id)
        content = ""
        if hasattr(incoming_message, 'text') and incoming_message.text:
            if hasattr(incoming_message.text, 'content'):
                content = incoming_message.text.content
        sender = getattr(incoming_message, 'sender_id', '')
        return f"{sender}_{hash(content)}"


def is_duplicate_message(msg_id: str) -> bool:
    """æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²å¤„ç†è¿‡"""
    if msg_id in PROCESSED_MESSAGES:
        return True
    
    # æ·»åŠ åˆ°å·²å¤„ç†é›†åˆ
    PROCESSED_MESSAGES.add(msg_id)
    
    # å¦‚æœè¶…è¿‡æœ€å¤§æ•°é‡ï¼Œæ¸…ç†æ—§çš„
    if len(PROCESSED_MESSAGES) > MAX_PROCESSED_MESSAGES:
        # ç®€å•æ¸…ç†ï¼šåˆ é™¤ä¸€åŠ
        to_remove = list(PROCESSED_MESSAGES)[:MAX_PROCESSED_MESSAGES // 2]
        for item in to_remove:
            PROCESSED_MESSAGES.discard(item)
    
    return False


def get_user_session(sender_id: str) -> dict:
    """è·å–ç”¨æˆ·ä¼šè¯ï¼Œè¿‡æœŸåˆ™è¿”å›ç©º"""
    session = USER_SESSIONS.get(sender_id)
    if session:
        if time.time() - session.get("timestamp", 0) < SESSION_TIMEOUT:
            return session
        else:
            # ä¼šè¯è¿‡æœŸï¼Œåˆ é™¤
            del USER_SESSIONS[sender_id]
    return {}


def update_user_session(sender_id: str, course_type: str = None, course_id: str = None, topic: str = None, last_query: str = None):
    """æ›´æ–°ç”¨æˆ·ä¼šè¯"""
    session = USER_SESSIONS.get(sender_id, {})
    session["timestamp"] = time.time()
    
    if course_type:
        session["course_type"] = course_type
    if course_id:
        session["course_id"] = course_id
    if topic:
        session["topic"] = topic
    if last_query:
        session["last_query"] = last_query
    
    USER_SESSIONS[sender_id] = session


def extract_topic_from_content(content: str, course_id: str = None) -> str:
    """ä»å†…å®¹ä¸­æå–è¯¾ç¨‹ä¸»é¢˜"""
    # å¸¸è§è¯¾ç¨‹åç§°æ˜ å°„
    topic_patterns = [
        (r"è‡ªåˆ¶è¡¨æƒ…åŒ…", "è‡ªåˆ¶è¡¨æƒ…åŒ…"),
        (r"ç‹¬ä¸€æ— äºŒçš„æˆ‘", "ç‹¬ä¸€æ— äºŒçš„æˆ‘"),
        (r"çœ¼çƒçš„å¥¥ç§˜", "çœ¼çƒçš„å¥¥ç§˜"),
        (r"èˆŒå°–çš„æ—…è¡Œ", "èˆŒå°–çš„æ—…è¡Œ"),
        (r"å°é¼»å­å¤§æœ¬äº‹", "å°é¼»å­å¤§æœ¬äº‹"),
        (r"å¥‡å¦™å¤§è€³æœµ", "å¥‡å¦™å¤§è€³æœµ"),
        (r"è¶…çº§èƒ½æ‰‹", "è¶…çº§èƒ½æ‰‹"),
        (r"æ‘‡æ‘†çš„èº«ä½“", "æ‘‡æ‘†çš„èº«ä½“"),
        (r"æ·±å‘¼å¸", "æ·±å‘¼å¸"),
        (r"æˆ‘çš„å°å¿ƒè„", "æˆ‘çš„å°å¿ƒè„"),
        (r"æ€•é…¸çš„ç‰™é½¿", "æ€•é…¸çš„ç‰™é½¿"),
        (r"äººä½“è¿·å®«", "äººä½“è¿·å®«"),
    ]
    
    for pattern, topic in topic_patterns:
        if re.search(pattern, content):
            return topic
    
    return None


def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(__file__).parent / "config.json"
    if config_path.exists():
        with open(config_path, "r", encoding="utf-8") as f:
            loaded = json.load(f)
            CONFIG.update(loaded)

    if not CONFIG["kb_path"]:
        CONFIG["kb_path"] = str(Path(__file__).parent / "knowledge_base")


def build_content_from_sections(sections: list) -> str:
    """å°†sectionsåˆå¹¶ä¸ºå¯æ£€ç´¢çš„æ­£æ–‡å†…å®¹"""
    parts = []
    for sec in sections or []:
        if not isinstance(sec, dict):
            continue
        title = (sec.get("title") or "").strip()
        content = (sec.get("content") or "").strip()
        if not title and not content:
            continue
        if title:
            parts.append(f"## {title}\n{content}".strip())
        else:
            parts.append(content)
    return "\n\n".join([p for p in parts if p])


def load_knowledge_base():
    """åŠ è½½çŸ¥è¯†åº“æ‰€æœ‰æ–‡æ¡£"""
    kb_dir = Path(CONFIG["kb_path"])
    documents = []

    if not kb_dir.exists():
        logger.error(f"çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {kb_dir}")
        return []

    for json_file in kb_dir.glob("*.json"):
        if json_file.name == "_index.json":
            continue

        try:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)

            if "entries" in data:
                for entry in data["entries"]:
                    documents.append({
                        "title": entry.get("title", ""),
                        "source": json_file.name,
                        "content": entry.get("content", {}).get("raw", ""),
                        "sections": []
                    })
            elif "title" in data:
                sections = data.get("sections", [])
                content = data.get("full_content") or build_content_from_sections(sections) or data.get("content", "")
                if content:
                    documents.append({
                        "title": data.get("title", ""),
                        "source": data.get("source", json_file.name),
                        "content": content,
                        "sections": sections
                    })
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½ {json_file.name}: {e}")

    logger.info(f"å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    return documents


# ============== çŸ¥è¯†åº“æœç´¢ ==============

def extract_query_terms(query: str) -> list[str]:
    """æå–æŸ¥è¯¢å…³é”®è¯ï¼ˆæ”¯æŒä¸­æ–‡ã€æ•°å­—ã€è¯¾ç¨‹ç¼–å·ï¼‰"""
    query_lower = query.lower()
    terms = set()

    # è‹±æ–‡/æ•°å­—è¿ç»­ç‰‡æ®µ
    for token in re.findall(r"[a-z0-9]+", query_lower):
        terms.add(token)

    # è¯¾ç¨‹ç¼–å·ï¼ˆå¦‚ 1-1-2ï¼‰
    for token in re.findall(r"\d+(?:-\d+)+", query_lower):
        terms.add(token)
        parts = token.split("-")
        if len(parts) >= 2:
            terms.add("-".join(parts[:2]))

    # ä¸­æ–‡è¿ç»­ç‰‡æ®µä¸äºŒå­—åˆ‡åˆ†
    for token in re.findall(r"[\u4e00-\u9fff]+", query_lower):
        terms.add(token)
        if len(token) >= 2:
            for i in range(len(token) - 1):
                terms.add(token[i:i + 2])

    if not terms:
        terms.add(query_lower.strip())

    return list(terms)


def extract_snippet(content: str, terms: list[str], before: int = 400, after: int = 1200) -> str | None:
    """ä»å†…å®¹ä¸­æˆªå–åŒ…å«å…³é”®è¯çš„ç‰‡æ®µ"""
    if not content or not terms:
        return None
    content_lower = content.lower()
    best = None  # (count, pos)
    best_pos = None
    for term in terms:
        if not term:
            continue
        if len(term) < 2:
            continue
        pos = content_lower.find(term)
        if pos == -1:
            continue
        count = content_lower.count(term)
        candidate = (count, pos)
        if best is None or candidate < best:
            best = candidate
            best_pos = pos
    if best_pos is None:
        return None
    start = max(best_pos - before, 0)
    end = min(best_pos + after, len(content))
    return content[start:end]


def extract_course_id(query: str) -> str | None:
    """æå–è¯¾ç¨‹ç¼–å·ï¼ˆå¦‚ 1-1-2ï¼‰"""
    match = re.search(r"\d+(?:-\d+)+", query)
    if match:
        return match.group(0)
    return None


def normalize_course_id(course_id: str) -> list[str]:
    """ç”Ÿæˆè¯¾ç¨‹ç¼–å·çš„æ‰€æœ‰å¯èƒ½å˜ä½“ï¼ˆå¤„ç†å‰å¯¼é›¶ï¼‰
    ä¾‹å¦‚ '1-1-2' -> ['1-1-2', '1-1-02', '1-01-2', '1-01-02', '01-1-2', ...]
    """
    if not course_id:
        return []
    
    parts = course_id.split("-")
    variants = set()
    
    # åŸå§‹ç‰ˆæœ¬
    variants.add(course_id)
    
    # ç”Ÿæˆå¸¦å‰å¯¼é›¶å’Œä¸å¸¦å‰å¯¼é›¶çš„å˜ä½“
    def generate_variants(parts_list, index=0, current=[]):
        if index == len(parts_list):
            variants.add("-".join(current))
            return
        
        part = parts_list[index]
        # ä¸å¸¦å‰å¯¼é›¶
        current.append(part.lstrip("0") or "0")
        generate_variants(parts_list, index + 1, current)
        current.pop()
        
        # å¸¦å‰å¯¼é›¶ï¼ˆä¸¤ä½æ•°æ ¼å¼ï¼‰
        if len(part) == 1:
            current.append("0" + part)
            generate_variants(parts_list, index + 1, current)
            current.pop()
    
    generate_variants(parts)
    return list(variants)


def detect_course_type(query: str) -> str | None:
    """æ£€æµ‹æŸ¥è¯¢ä¸­çš„è¯¾ç¨‹ç±»å‹"""
    query_lower = query.lower()
    
    # STEMå¹¼å„¿è¯¾ç¨‹å…³é”®è¯
    if any(kw in query_lower for kw in ["å°ç­", "ä¸­ç­", "å¤§ç­", "å¹¼å„¿", "stem", "3å²", "4å²", "5å²", "6å²", "è®¤è¯†æˆ‘è‡ªå·±", "åŠ¨ç‰©ç‹å›½", "æ¤ç‰©å¥¥ç§˜", "æ•°ç†ç‰©ç†", "æœºæ¢°ä¸å·¥å…·", "å»ºç­‘ä¸ç»“æ„", "æ™ºèƒ½æœºæ¢°", "ç‰©ç†ç§‘å­¦", "å¤æ‚æœºæ¢°", "åœ°çƒä¸ç©ºé—´", "èƒ½æºç§‘å­¦", "æ™ºèƒ½ç¡¬ä»¶"]):
        return "STEM"
    
    # PythonAIè¯¾ç¨‹å…³é”®è¯
    if any(kw in query_lower for kw in ["python", "pythonai", "äººå·¥æ™ºèƒ½", "aiè¯¾", "l1", "l2", "å‡½æ•°", "ç®—æ³•", "æ•°æ®ç»“æ„", "è®¡ç®—æœºè§†è§‰", "ä»¿ç”Ÿ"]):
        return "PythonAI"
    
    # CODEå°‘å„¿ç¼–ç¨‹å…³é”®è¯
    if any(kw in query_lower for kw in ["code1", "code2", "code3", "scratch", "å°‘å„¿ç¼–ç¨‹", "ç¼–ç¨‹å¯è’™", "æ¸¸æˆå¼€å‘"]):
        return "CODE"
    
    # C++ä¿¡å¥¥å…³é”®è¯
    if any(kw in query_lower for kw in ["ä¿¡å¥¥", "c++", "noi", "csp", "ç«èµ›"]):
        return "CPP"
    
    return None


def filter_documents_by_type(documents: list, course_type: str) -> list:
    """æ ¹æ®è¯¾ç¨‹ç±»å‹è¿‡æ»¤æ–‡æ¡£"""
    if not course_type:
        return documents
    
    filtered = []
    for doc in documents:
        title = doc.get("title", "").upper()
        source = doc.get("source", "").upper()
        
        if course_type == "STEM":
            # STEMæ–‡æ¡£ï¼šæ ‡é¢˜æˆ–æ¥æºåŒ…å«STEM/å°ç­/ä¸­ç­/å¤§ç­
            if any(kw in title or kw in source for kw in ["STEM", "å°ç­", "ä¸­ç­", "å¤§ç­"]):
                filtered.append(doc)
        elif course_type == "PythonAI":
            if "PYTHON" in title or "PYTHON" in source:
                filtered.append(doc)
        elif course_type == "CODE":
            if "CODE" in title or "CODE" in source:
                filtered.append(doc)
        elif course_type == "CPP":
            if any(kw in title or kw in source for kw in ["C++", "ä¿¡å¥¥", "NOI", "CSP"]):
                filtered.append(doc)
    
    return filtered if filtered else documents  # å¦‚æœè¿‡æ»¤åä¸ºç©ºï¼Œè¿”å›å…¨éƒ¨


def find_course_matches(course_id: str, documents: list, course_type: str = None) -> list[dict]:
    """æŒ‰è¯¾ç¨‹ç¼–å·ç²¾ç¡®åŒ¹é…æ–‡æ¡£ï¼Œç”ŸæˆåŒ…å«ç‰‡æ®µçš„ä¸Šä¸‹æ–‡"""
    matches = []
    
    # ç”Ÿæˆè¯¾ç¨‹ç¼–å·çš„æ‰€æœ‰å˜ä½“ï¼ˆå¤„ç†å‰å¯¼é›¶é—®é¢˜ï¼‰
    course_id_variants = normalize_course_id(course_id)
    
    # åˆ›å»ºåŒ¹é…æ‰€æœ‰å˜ä½“çš„æ­£åˆ™è¡¨è¾¾å¼
    pattern_str = "|".join(re.escape(v) for v in course_id_variants)
    pattern = re.compile(pattern_str)
    
    for doc in documents:
        found = False
        
        # 1. å…ˆå°è¯•åœ¨contentä¸­åŒ¹é…
        content = doc.get("content", "")
        if content:
            hit = pattern.search(content)
            if hit:
                start = max(hit.start() - 600, 0)
                end = min(hit.end() + 1200, len(content))
                snippet = content[start:end]
                matches.append({
                    "title": doc.get("title", "æœªçŸ¥"),
                    "source": doc.get("source", ""),
                    "content": snippet
                })
                found = True

        # 2. åœ¨ sections ä¸­åŒ¹é…ï¼ˆæ— è®ºcontentæ˜¯å¦æœ‰å†…å®¹ï¼‰
        if not found:
            sections = doc.get("sections", [])
            for sec in sections:
                if not isinstance(sec, dict):
                    continue
                sec_content = sec.get("content", "")
                hit = pattern.search(sec_content)
                if hit:
                    # æå–åŒ¹é…ç‚¹é™„è¿‘çš„å†…å®¹
                    start = max(hit.start() - 300, 0)
                    end = min(hit.end() + 1500, len(sec_content))
                    snippet = sec_content[start:end]
                    matches.append({
                        "title": f"{doc.get('title', 'æœªçŸ¥')} - {sec.get('title', '')}",
                        "source": doc.get("source", ""),
                        "content": snippet
                    })
                    break
        
        # 3. ä¹Ÿæ£€æŸ¥ full_content å­—æ®µ
        if not found:
            full_content = doc.get("full_content", "")
            if full_content:
                hit = pattern.search(full_content)
                if hit:
                    start = max(hit.start() - 600, 0)
                    end = min(hit.end() + 1500, len(full_content))
                    snippet = full_content[start:end]
                    matches.append({
                        "title": doc.get("title", "æœªçŸ¥"),
                        "source": doc.get("source", ""),
                        "content": snippet
                    })

    return matches


def search_documents(query: str, documents: list, max_results: int = 5) -> list:
    """æœç´¢ç›¸å…³æ–‡æ¡£"""
    query_lower = query.lower()
    query_terms = extract_query_terms(query)
    results = []

    for doc in documents:
        score = 0
        title_raw = doc.get("title", "")
        content_raw = doc.get("content", "")
        sections = doc.get("sections", [])
        section_text = []
        if sections:
            for sec in sections:
                if isinstance(sec, dict):
                    section_text.append(sec.get("title", ""))
                    section_text.append(sec.get("content", ""))
        content_raw_combined = content_raw
        if section_text:
            content_raw_combined = content_raw + "\n" + "\n".join(section_text)

        title = title_raw.lower()
        content = content_raw_combined.lower()

        for term in query_terms:
            if term in title:
                score += 10
            if term in content:
                score += 3 + content.count(term)

        # æ–¯å¦æ˜Ÿçƒä¸“ç”¨å…³é”®è¯åŠ æƒ
        keywords = {
            "stem": ["stem", "å¹¼å„¿", "ç§‘åˆ›", "æœºæ¢°", "å»ºç­‘", "ç‰©ç†"],
            "å°ç­": ["å°ç­", "3-4å²", "è®¤è¯†æˆ‘è‡ªå·±", "åŠ¨ç‰©", "æ¤ç‰©"],
            "ä¸­ç­": ["ä¸­ç­", "4-5å²", "æœºæ¢°", "å»ºç­‘", "æ™ºèƒ½"],
            "å¤§ç­": ["å¤§ç­", "5-6å²", "å¤æ‚æœºæ¢°", "èƒ½æº", "ç©ºé—´", "æ™ºèƒ½ç¡¬ä»¶"],
            "code": ["code", "scratch", "ç¼–ç¨‹", "å°‘å„¿ç¼–ç¨‹", "æ¸¸æˆå¼€å‘"],
            "code1": ["code1", "æœºæ¢°ç»“æ„", "æ™ºèƒ½ç¡¬ä»¶", "ç¼–ç¨‹å¯è’™"],
            "code2": ["code2", "æ™ºèƒ½åº”ç”¨", "æ™ºèƒ½äº¤äº’", "ç®—æ³•é€»è¾‘"],
            "code3": ["code3", "æ™ºèƒ½ç³»ç»Ÿ", "æ¸¸æˆå¼€å‘", "é«˜çº§å·¥ç¨‹"],
            "python": ["python", "pythonai", "äººå·¥æ™ºèƒ½", "ai"],
            "l1": ["l1", "å‡½æ•°", "ç®—æ³•", "æ•°æ®ç»“æ„"],
            "l2": ["l2", "æ•°æ®ç§‘å­¦", "è®¡ç®—æœºè§†è§‰", "cv", "ä»¿ç”Ÿ"],
            "ä¿¡å¥¥": ["ä¿¡å¥¥", "c++", "noi", "csp", "ç«èµ›"],
            "é”€å”®": ["é”€å”®", "è¯æœ¯", "å’¨è¯¢", "å¼‚è®®", "ä¿ƒå•"],
            "å®¶é•¿": ["å®¶é•¿", "æ²Ÿé€š", "ç»­è´¹", "è½¬ä»‹ç»"],
        }

        for key, terms in keywords.items():
            if any(t in query_lower for t in terms):
                if any(t in title or t in content for t in terms):
                    score += 5

        if score > 0:
            doc["_score"] = score
            snippet = extract_snippet(content_raw_combined, query_terms)
            if snippet:
                doc["_snippet"] = snippet
            results.append(doc)

    results.sort(key=lambda x: x.get("_score", 0), reverse=True)
    return results[:max_results]


def build_context(documents: list, max_chars: int = 8000) -> str:
    """æ„å»ºä¸Šä¸‹æ–‡"""
    context_parts = []
    total_chars = 0

    for doc in documents:
        title = doc.get("title", "æœªçŸ¥")
        content = doc.get("_snippet") or doc.get("content", "")

        if total_chars + len(content) > max_chars:
            remaining = max_chars - total_chars
            if remaining > 500:
                content = content[:remaining] + "\n...(å†…å®¹æˆªæ–­)"
            else:
                break

        context_parts.append(f"### {title}\n\n{content}")
        total_chars += len(content)

    return "\n\n---\n\n".join(context_parts)


# ============== LLM API ==============

def get_llm_config():
    """è·å–å¤§æ¨¡å‹é…ç½®ï¼ˆä¼˜å…ˆè¯»å–llm_*, å…¼å®¹claude_*ï¼‰"""
    api_key = CONFIG.get("llm_api_key") or CONFIG.get("claude_api_key") or ""
    base_url = CONFIG.get("llm_base_url") or CONFIG.get("claude_base_url") or "https://open.bigmodel.cn/api/paas/v4"
    model = CONFIG.get("llm_model") or "glm-4.7"
    return api_key, base_url, model


def clean_markdown(text: str) -> str:
    """æ¸…ç†Markdownæ ¼å¼ç¬¦å·ï¼Œè½¬ä¸ºçº¯æ–‡æœ¬"""
    if not text:
        return text
    
    # ç§»é™¤ä»£ç å—
    text = re.sub(r'```[\s\S]*?```', lambda m: m.group(0).replace('```', '').strip(), text)
    
    # ç§»é™¤è¡Œå†…ä»£ç 
    text = re.sub(r'`([^`]+)`', r'\1', text)
    
    # ç§»é™¤åŠ ç²—
    text = re.sub(r'\*\*([^*]+)\*\*', r'ã€\1ã€‘', text)
    text = re.sub(r'__([^_]+)__', r'ã€\1ã€‘', text)
    
    # ç§»é™¤æ–œä½“
    text = re.sub(r'\*([^*]+)\*', r'\1', text)
    text = re.sub(r'_([^_]+)_', r'\1', text)
    
    # ç§»é™¤æ ‡é¢˜ç¬¦å·
    text = re.sub(r'^#{1,6}\s*', '', text, flags=re.MULTILINE)
    
    # ç§»é™¤åˆ†éš”çº¿
    text = re.sub(r'^-{3,}$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\*{3,}$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^_{3,}$', '', text, flags=re.MULTILINE)
    
    # å°†Markdownåˆ—è¡¨ç¬¦å·æ›¿æ¢ä¸ºæ›´å‹å¥½çš„ç¬¦å·
    text = re.sub(r'^[-*+]\s+', 'Â· ', text, flags=re.MULTILINE)
    
    # ç§»é™¤é“¾æ¥æ ¼å¼ï¼Œä¿ç•™æ–‡å­—
    text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)
    
    # æ¸…ç†å¤šä½™ç©ºè¡Œ
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()


def ask_llm(question: str, context: str) -> str:
    """è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”ï¼ˆæ™ºè°±OpenAIå…¼å®¹æ¥å£ï¼‰"""
    api_key, base_url, model = get_llm_config()
    if not api_key:
        return "é”™è¯¯ï¼šæœªé…ç½®å¤§æ¨¡å‹APIå¯†é’¥"

    system_prompt = """ä½ æ˜¯æ–¯å¦æ˜Ÿçƒçš„çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”è€å¸ˆå’Œé”€å”®é¡¾é—®å…³äºè¯¾ç¨‹ã€æ•™å­¦ã€é”€å”®çš„é—®é¢˜ã€‚

æ–¯å¦æ˜Ÿçƒç®€ä»‹ï¼š
- ä¸“æ³¨äºSTEMç§‘åˆ›å’Œç¼–ç¨‹æ•™è‚²
- è¯¾ç¨‹ä½“ç³»ï¼šSTEMå¹¼å„¿ç§‘åˆ›ï¼ˆ3-6å²ï¼‰â†’ CODEå°‘å„¿ç¼–ç¨‹ï¼ˆ6-12å²ï¼‰â†’ PythonAIï¼ˆ10å²+ï¼‰â†’ C++ä¿¡å¥¥
- æ•™å­¦ç†å¿µï¼šé¡¹ç›®åˆ¶å­¦ä¹ (PBL)ã€å…«å¤§èƒ½åŠ›åŸ¹å…»ã€åšä¸­å­¦

é‡è¦è§„åˆ™ï¼š
1. åªèƒ½åŸºäºæä¾›çš„çŸ¥è¯†åº“å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ ä»»ä½•ä¿¡æ¯
2. å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œæ˜ç¡®è¯´"è¿™ä¸ªé—®é¢˜æˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œå»ºè®®å’¨è¯¢æ•™å­¦ä¸»ç®¡"
3. å›ç­”è¦ç®€æ´å®ç”¨ï¼Œç›´æ¥ç»™å‡ºç­”æ¡ˆ
4. æ¶‰åŠå…·ä½“è¯¾ç¨‹ã€å¹´é¾„ã€çº§åˆ«æ—¶ï¼Œå¿…é¡»ä¸¥æ ¼æŒ‰ç…§çŸ¥è¯†åº“å†…å®¹
5. ç”¨å£è¯­åŒ–çš„æ–¹å¼å›ç­”ï¼ŒåƒåŒäº‹ä¹‹é—´çš„å¯¹è¯

æ ¼å¼è¦æ±‚ï¼ˆéå¸¸é‡è¦ï¼ï¼‰ï¼š
- è¾“å‡ºçº¯æ–‡æœ¬ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•Markdownæ ¼å¼
- ä¸è¦ç”¨ **åŠ ç²—** æˆ– *æ–œä½“*
- ä¸è¦ç”¨ ## æ ‡é¢˜ æˆ– ### å°æ ‡é¢˜
- ä¸è¦ç”¨ ``` ä»£ç å—
- ä¸è¦ç”¨ --- åˆ†éš”çº¿
- ä¸è¦ç”¨ | è¡¨æ ¼ |
- ç”¨ã€ã€‘æˆ–ã€Œã€æ¥å¼ºè°ƒé‡ç‚¹ï¼Œç”¨ç©ºè¡Œåˆ†æ®µ
- ç”¨æ•°å­—1. 2. 3.æˆ–ç¬¦å·â€¢æ¥åˆ—ä¸¾ï¼Œä¸è¦ç”¨-

è¯¾ç¨‹ä½“ç³»è¦ç‚¹ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
- STEMï¼šå°ç­(3-4å²)â†’ä¸­ç­(4-5å²)â†’å¤§ç­(5-6å²)ï¼Œæ¯é˜¶æ®µ4ä¸ªä¸»é¢˜
- CODEï¼šCODE1(6-8å²)â†’CODE2(8-10å²)â†’CODE3(10-12å²)ï¼Œæœºæ¢°+ç¼–ç¨‹ç»“åˆ
- PythonAIï¼šL1(10-12å²)â†’L2(12å²+)ï¼Œäººå·¥æ™ºèƒ½æ–¹å‘
- C++ä¿¡å¥¥ï¼šé¢å‘ç«èµ›çš„ä¸“ä¸šè¯¾ç¨‹"""

    user_message = f"""è¯·åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ã€‚

ã€çŸ¥è¯†åº“å†…å®¹ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

è¯·ç›´æ¥å›ç­”ï¼Œä¸è¦è¯´"æ ¹æ®çŸ¥è¯†åº“"ä¹‹ç±»çš„å¼€åœºç™½ã€‚"""

    url = base_url.rstrip("/") + "/chat/completions"
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        "temperature": 0.2,
        "max_tokens": 1200,
        "thinking": {"type": "disabled"},
    }
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    try:
        resp = requests.post(url, json=payload, headers=headers, timeout=30)
        if resp.status_code != 200:
            logger.error(f"LLMé”™è¯¯: {resp.status_code} {resp.text[:300]}")
            return "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
        data = resp.json()
        content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
        if not content:
            logger.error(f"LLMè¿”å›ç©ºå†…å®¹: {data}")
            return "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
        return clean_markdown(content)
    except Exception as e:
        logger.exception(f"LLMè°ƒç”¨å¼‚å¸¸: {type(e).__name__}: {e}")
        return f"æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚(é”™è¯¯: {type(e).__name__})"


def is_follow_up_query(question: str) -> bool:
    """æ£€æµ‹æ˜¯å¦æ˜¯è·Ÿè¿›æ€§é—®é¢˜ï¼ˆéœ€è¦ä¸Šä¸‹æ–‡çš„æ¨¡ç³ŠæŸ¥è¯¢ï¼‰"""
    follow_up_patterns = [
        r"^å±•å¼€", r"^è¯¦ç»†", r"^ç»§ç»­", r"^å†è¯´è¯´", r"^è¿˜æœ‰å—", r"^æ›´å¤š",
        r"^ç»†è¯´", r"^å…·ä½“", r"æ€ä¹ˆ[åŠåš]", r"^æ¥ç€è¯´", r"^ç„¶åå‘¢",
        r"^è¿˜èƒ½.*å—", r"^å¯ä»¥.*å—", r"^èƒ½ä¸èƒ½", r"^å¸®æˆ‘.*å±•å¼€",
        r"^è¯´è¯¦ç»†", r"^è®²è®²", r"^èŠèŠ",
    ]
    question_lower = question.strip()
    for pattern in follow_up_patterns:
        if re.search(pattern, question_lower):
            return True
    # å¤ªçŸ­çš„é—®é¢˜å¯èƒ½æ˜¯è·Ÿè¿›
    if len(question_lower) < 10 and not extract_course_id(question_lower):
        return True
    return False


def process_question(question: str, sender_id: str = "") -> str:
    """å¤„ç†ç”¨æˆ·é—®é¢˜"""
    
    # 0. è·å–ç”¨æˆ·ä¼šè¯ä¸Šä¸‹æ–‡
    session = get_user_session(sender_id) if sender_id else {}
    
    # 1. æ£€æµ‹è¯¾ç¨‹ç±»å‹ï¼ˆå°ç­/ä¸­ç­/å¤§ç­/CODE/Pythonç­‰ï¼‰
    course_type = detect_course_type(question)
    course_id = extract_course_id(question)
    
    # 2. æ£€æµ‹æ˜¯å¦æ˜¯è·Ÿè¿›æ€§é—®é¢˜
    if is_follow_up_query(question) and not course_type and not course_id:
        # ä»ä¼šè¯ä¸­æ¢å¤ä¸Šä¸‹æ–‡
        if session:
            course_type = session.get("course_type")
            course_id = session.get("course_id")
            topic = session.get("topic")
            last_query = session.get("last_query", "")
            
            if course_type or course_id:
                logger.info(f"è·Ÿè¿›æŸ¥è¯¢ï¼Œä½¿ç”¨ä¼šè¯ä¸Šä¸‹æ–‡: type={course_type}, id={course_id}, topic={topic}")
                # å°†ä¸Šä¸‹æ–‡ä¿¡æ¯è¡¥å……åˆ°é—®é¢˜ä¸­
                if topic:
                    question = f"å…³äº{topic}ï¼Œ{question}"
                elif course_id:
                    question = f"å…³äºè¯¾ç¨‹{course_id}ï¼Œ{question}"
    
    # 3. æ ¹æ®è¯¾ç¨‹ç±»å‹é¢„å…ˆè¿‡æ»¤æ–‡æ¡£èŒƒå›´
    filtered_docs = filter_documents_by_type(KB_DOCUMENTS, course_type)
    
    # 4. æå–è¯¾ç¨‹ç¼–å·å¹¶åœ¨è¿‡æ»¤åçš„èŒƒå›´å†…æœç´¢
    if course_id:
        course_docs = find_course_matches(course_id, filtered_docs, course_type)
        if course_docs:
            context = build_context(course_docs, max_chars=8000)
            
            # å°è¯•ä»ä¸Šä¸‹æ–‡ä¸­æå–ä¸»é¢˜
            topic = extract_topic_from_content(context, course_id)
            
            # æ›´æ–°ä¼šè¯
            if sender_id:
                update_user_session(sender_id, course_type, course_id, topic, question)
            
            return ask_llm(question, context)

    # 5. å¦‚æœæ²¡æœ‰è¯¾ç¨‹ç¼–å·åŒ¹é…ï¼Œç”¨å…³é”®è¯æœç´¢
    relevant_docs = search_documents(question, filtered_docs, max_results=5)

    if not relevant_docs:
        return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚è¯·å°è¯•æ¢ä¸ªå…³é”®è¯ï¼Œæˆ–å’¨è¯¢æ•™å­¦ä¸»ç®¡ã€‚"

    context = build_context(relevant_docs)
    
    # æ›´æ–°ä¼šè¯
    if sender_id:
        topic = extract_topic_from_content(context)
        update_user_session(sender_id, course_type, course_id, topic, question)
    
    return ask_llm(question, context)


# ============== å¿«æ·å‘½ä»¤ ==============

def handle_shortcut(cmd: str) -> str:
    """å¤„ç†å¿«æ·å‘½ä»¤"""
    cmd = cmd.lower()
    
    if cmd == "stem":
        return """ğŸ“˜ STEMå¹¼å„¿ç§‘åˆ›è¯¾ç¨‹ï¼ˆ3-6å²ï¼‰

ã€å°ç­ 3-4å²ã€‘
é˜¶æ®µ1ï¼šè®¤è¯†æˆ‘è‡ªå·± - èº«ä½“éƒ¨ä½ã€æ„Ÿå®˜æ¢ç´¢
é˜¶æ®µ2ï¼šåŠ¨ç‰©ç‹å›½ - åŠ¨ç‰©ç‰¹å¾ã€ä»¿ç”Ÿè®¾è®¡
é˜¶æ®µ3ï¼šæ¤ç‰©å¥¥ç§˜ - æ¤ç‰©ç”Ÿé•¿ã€è§‚å¯Ÿè®°å½•
é˜¶æ®µ4ï¼šæ•°ç†ç‰©ç† - åŸºç¡€ç‰©ç†ã€æ•°å­¦å¯è’™

ã€ä¸­ç­ 4-5å²ã€‘
é˜¶æ®µ1ï¼šæœºæ¢°ä¸å·¥å…· - æ æ†ã€æ»‘è½®ã€é½¿è½®
é˜¶æ®µ2ï¼šå»ºç­‘ä¸ç»“æ„ - æ¡¥æ¢ã€å¡”æ¥¼ã€ç¨³å®šæ€§
é˜¶æ®µ3ï¼šæ™ºèƒ½æœºæ¢° - ç”µåŠ¨é©¬è¾¾ã€ä¼ åŠ¨ç³»ç»Ÿ
é˜¶æ®µ4ï¼šç‰©ç†ç§‘å­¦ - å£°å…‰ç”µçƒ­æ¢ç´¢

ã€å¤§ç­ 5-6å²ã€‘
é˜¶æ®µ1ï¼šå¤æ‚æœºæ¢° - ç»¼åˆæœºæ¢°ç³»ç»Ÿ
é˜¶æ®µ2ï¼šåœ°çƒä¸ç©ºé—´ - å¤©æ–‡ã€åœ°ç†
é˜¶æ®µ3ï¼šèƒ½æºç§‘å­¦ - æ–°èƒ½æºã€ç”µè·¯
é˜¶æ®µ4ï¼šæ™ºèƒ½ç¡¬ä»¶ - ç¼–ç¨‹åˆä½“éªŒ

ğŸ’¡ æƒ³äº†è§£æ›´å¤šï¼Ÿå¯ä»¥é—®æˆ‘å…·ä½“è¯¾æ—¶å†…å®¹"""

    elif cmd == "code":
        return """ğŸ’» CODEå°‘å„¿ç¼–ç¨‹è¯¾ç¨‹ï¼ˆ6-12å²ï¼‰

ã€CODE1 6-8å²ã€‘ä¹é«˜+Scratchå¯è’™
â€¢ æ¨¡å—1ï¼šæœºæ¢°ç»“æ„åŸºç¡€
â€¢ æ¨¡å—2ï¼šæ™ºèƒ½ç¡¬ä»¶ä¸ä»¿ç”Ÿ
â€¢ æ¨¡å—3ï¼šScratchç¼–ç¨‹å¯è’™
â€¢ æ¨¡å—4ï¼šç¼–ç¨‹é€»è¾‘è¿›é˜¶

ã€CODE2 8-10å²ã€‘è¿›é˜¶ç¼–ç¨‹
â€¢ æ¨¡å—1ï¼šæ™ºèƒ½ç”Ÿæ´»åº”ç”¨
â€¢ æ¨¡å—2ï¼šæ™ºèƒ½äº¤äº’ç³»ç»Ÿ
â€¢ æ¨¡å—3ï¼šç®—æ³•ä¸æ•°å­¦é€»è¾‘

ã€CODE3 10-12å²ã€‘é«˜çº§å·¥ç¨‹
â€¢ æ¨¡å—1ï¼šæ™ºèƒ½ç³»ç»Ÿè®¾è®¡
â€¢ æ¨¡å—2ï¼šæ¸¸æˆå¼€å‘PBL
â€¢ æ¨¡å—3ï¼šé«˜çº§æœºæ¢°å·¥ç¨‹

ç‰¹è‰²ï¼šæœºæ¢°æ­å»º+å›¾å½¢åŒ–ç¼–ç¨‹+é€»è¾‘æ€ç»´

ğŸ’¡ æƒ³äº†è§£å‡ç­è§„åˆ™ï¼Ÿé—®æˆ‘ CODEæ€ä¹ˆå‡ç­"""

    elif cmd == "python":
        return """ğŸ PythonAIè¯¾ç¨‹ï¼ˆ10å²+ï¼‰

ã€L1é˜¶æ®µ 10-12å²ã€‘PythonåŸºç¡€+AIå¯è’™
â€¢ æ¨¡å—0ï¼šåŸºç¡€è¯­æ³•ä¸é€»è¾‘å¯è’™
â€¢ æ¨¡å—1ï¼šå‡½æ•°å°è£…ä¸äº¤äº’æœºåˆ¶
â€¢ æ¨¡å—2ï¼šç®—æ³•é€»è¾‘ä¸æ•°å€¼è¿ç®—
â€¢ æ¨¡å—3ï¼šæ•°æ®ç»“æ„ä¸å¤æ‚é€»è¾‘
â€¢ æ¨¡å—4ï¼šæ™ºèƒ½ç¡¬ä»¶ä¸AIåº”ç”¨

ã€L2é˜¶æ®µ 12å²+ã€‘AIè¿›é˜¶
â€¢ æ¨¡å—1ï¼šç®—æ³•ä¸æ•°æ®ç§‘å­¦
â€¢ æ¨¡å—2ï¼šAIå¯¹è¯ä¸æ™ºèƒ½ä½“
â€¢ æ¨¡å—3ï¼šäº¤äº’å¼AIä¸ä»¿ç”Ÿæ§åˆ¶
â€¢ æ¨¡å—4ï¼šè®¡ç®—æœºè§†è§‰(CV)

ç‰¹è‰²ï¼šå®æˆ˜é¡¹ç›®é©±åŠ¨ã€AIåº”ç”¨å¼€å‘

ğŸ’¡ æƒ³äº†è§£å…¥å­¦è¯„ä¼°ï¼Ÿé—®æˆ‘ Pythonæ€ä¹ˆæµ‹è¯„"""

    elif cmd in ["ä»·æ ¼", "ä¿ƒå•"]:
        return """ğŸ’° å¸¸è§ä»·æ ¼å¼‚è®®å¤„ç†

ã€å¤ªè´µäº†ã€‘
âœ… è®¤åŒ â†’ æ‹†åˆ†ä»·å€¼ â†’ å¯¹æ¯”æŠ•å…¥
"ç†è§£æ‚¨çš„é¡¾è™‘ã€‚å’±ä»¬æ‹†å¼€ç®—ä¸€ä¸‹ï¼ŒXXè¯¾æ—¶å¹³å‡æ¯æ¬¡è¯¾XXå…ƒï¼Œä¸€å‘¨Xæ¬¡ï¼Œå­¦ä¸‹æ¥å­©å­èƒ½è·å¾—..."

ã€å†è€ƒè™‘ã€‘
âœ… è®¤åŒ â†’ æ¢è¯¢çœŸå®åŸå›  â†’ è§£å†³é¡¾è™‘
"å®Œå…¨ç†è§£ï¼Œé‚£æ‚¨ä¸»è¦æ˜¯æƒ³è€ƒè™‘å“ªæ–¹é¢å‘¢ï¼Ÿæ˜¯æ—¶é—´å®‰æ’è¿˜æ˜¯...ï¼Ÿ"

ã€åˆ«å®¶ä¾¿å®œã€‘
âœ… ä¸å¦å®š â†’ å·®å¼‚åŒ– â†’ ä»·å€¼é”šå®š
"æ˜¯çš„ï¼Œå¸‚é¢ä¸Šä»·æ ¼åŒºé—´å¾ˆå¤§ã€‚å’±ä»¬çš„ç‰¹ç‚¹æ˜¯...æ‚¨å¯ä»¥å¯¹æ¯”ä¸€ä¸‹è¯¾ç¨‹å†…å®¹å’Œå¸ˆèµ„..."

ğŸ’¡ æƒ³è¦æ›´å¤šè¯æœ¯ï¼Ÿé—®æˆ‘ å¼‚è®®å¤„ç†"""

    return None


# ============== å¤„ç†å•æ¡æ¶ˆæ¯ ==============

def handle_message(content: str, sender_nick: str, sender_id: str = "") -> str:
    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›å›å¤"""
    content = content.strip()
    
    if not content:
        return ""

    # å¸®åŠ©å‘½ä»¤
    if content in ["å¸®åŠ©", "help", "?"]:
        return """ğŸ¤– æ–¯å¦æ˜ŸçƒçŸ¥è¯†åº“åŠ©æ‰‹

ç›´æ¥è¾“å…¥é—®é¢˜å³å¯ï¼Œä¾‹å¦‚ï¼š
â€¢ STEMå°ç­å­¦ä»€ä¹ˆå†…å®¹ï¼Ÿ
â€¢ CODE1å’ŒCODE2æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
â€¢ å®¶é•¿é—®Pythonæœ‰ä»€ä¹ˆç”¨æ€ä¹ˆå›ç­”ï¼Ÿ
â€¢ å­©å­å¤šå¤§å¯ä»¥å­¦ç¼–ç¨‹ï¼Ÿ
â€¢ å®¶é•¿è¯´ä»·æ ¼è´µæ€ä¹ˆå¤„ç†ï¼Ÿ

ğŸ“š æ”¯æŒçš„çŸ¥è¯†é¢†åŸŸï¼š
â€¢ è¯¾ç¨‹ä½“ç³»ï¼ˆSTEM/CODE/PythonAI/C++ä¿¡å¥¥ï¼‰
â€¢ é”€å”®è¯æœ¯ä¸å¼‚è®®å¤„ç†
â€¢ æ•™å­¦æ–¹æ³•ä¸è¯¾å ‚ç®¡ç†
â€¢ å®¶é•¿æ²Ÿé€šæŠ€å·§

âš¡ å¿«æ·å‘½ä»¤ï¼š
â€¢ /stem - STEMè¯¾ç¨‹ä»‹ç»
â€¢ /code - CODEç¼–ç¨‹è¯¾ç¨‹
â€¢ /python - PythonAIè¯¾ç¨‹
â€¢ /ä»·æ ¼ - ä»·æ ¼å¼‚è®®å¤„ç†

ğŸ’¡ æç¤ºï¼šæ‚¨ä¹Ÿå¯ä»¥ç§èŠæˆ‘ï¼Œè·å¾—æ›´ä¸“æ³¨çš„æœåŠ¡"""

    # å¿«æ·å‘½ä»¤
    if content.startswith("/"):
        cmd = content[1:].strip()
        shortcut_reply = handle_shortcut(cmd)
        if shortcut_reply:
            return shortcut_reply

    # æ™®é€šé—®ç­”ï¼ˆä¼ å…¥sender_idç”¨äºä¼šè¯ç®¡ç†ï¼‰
    return process_question(content, sender_id)


# ============== é’‰é’‰æ¶ˆæ¯å¤„ç†å™¨ ==============

class StarplanetKnowledgeHandler(ChatbotHandler):
    """æ–¯å¦æ˜ŸçƒçŸ¥è¯†åº“æœºå™¨äººæ¶ˆæ¯å¤„ç†å™¨"""

    async def process(self, callback):
        """å¤„ç†æ”¶åˆ°çš„æ¶ˆæ¯"""
        try:
            incoming_message = callback.data
            logger.info(f"æ”¶åˆ°å›è°ƒ, ç±»å‹: {type(incoming_message)}")
            
            # ====== æ¶ˆæ¯å»é‡æ£€æŸ¥ ======
            msg_id = get_message_id(incoming_message)
            if is_duplicate_message(msg_id):
                logger.info(f"é‡å¤æ¶ˆæ¯ï¼Œè·³è¿‡: {msg_id}")
                return AckMessage.STATUS_OK, "OK"
            
            # å°è¯•è·å–æ¶ˆæ¯å†…å®¹
            content = ""
            sender_nick = "ç”¨æˆ·"
            sender_id = ""
            
            if isinstance(incoming_message, dict):
                # å­—å…¸æ ¼å¼
                text_obj = incoming_message.get("text", {})
                if isinstance(text_obj, dict):
                    content = text_obj.get("content", "")
                elif isinstance(text_obj, str):
                    content = text_obj
                sender_nick = incoming_message.get("senderNick", "ç”¨æˆ·")
                sender_id = incoming_message.get("senderId", "") or incoming_message.get("senderStaffId", "")
                logger.info(f"å­—å…¸æ ¼å¼ - ç”¨æˆ·: {sender_nick}, ID: {sender_id}, å†…å®¹: {content[:50] if content else '(ç©º)'}")
            else:
                # ChatbotMessageå¯¹è±¡
                if hasattr(incoming_message, 'text') and incoming_message.text:
                    if hasattr(incoming_message.text, 'content'):
                        content = incoming_message.text.content
                    else:
                        content = str(incoming_message.text)
                sender_nick = getattr(incoming_message, 'sender_nick', 'ç”¨æˆ·') or "ç”¨æˆ·"
                sender_id = getattr(incoming_message, 'sender_id', '') or getattr(incoming_message, 'sender_staff_id', '')
                logger.info(f"å¯¹è±¡æ ¼å¼ - ç”¨æˆ·: {sender_nick}, ID: {sender_id}, å†…å®¹: {content[:50] if content else '(ç©º)'}")
            
            content = content.strip() if content else ""
            
            if not content:
                logger.info("æ¶ˆæ¯å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                return AckMessage.STATUS_OK, "OK"
            
            # å¤„ç†æ¶ˆæ¯ï¼ˆä¼ å…¥sender_idç”¨äºä¼šè¯ç®¡ç†ï¼‰
            reply = await asyncio.to_thread(handle_message, content, sender_nick, sender_id)
            
            if reply:
                # æ ¹æ®æ¶ˆæ¯ç±»å‹é€‰æ‹©å›å¤æ–¹å¼
                if isinstance(incoming_message, dict):
                    # ä½¿ç”¨ChatbotMessageå¯¹è±¡å›å¤
                    message = ChatbotMessage.from_dict(incoming_message)
                    self.reply_text(reply, message)
                else:
                    self.reply_text(reply, incoming_message)
                logger.info(f"å·²å›å¤: {reply[:50]}...")
            
            return AckMessage.STATUS_OK, "OK"

        except Exception as e:
            logger.exception(f"å¤„ç†æ¶ˆæ¯å¼‚å¸¸: {e}")
            return AckMessage.STATUS_OK, "OK"


# ============== å¯åŠ¨ ==============

def check_single_instance():
    """ç¡®ä¿åªæœ‰ä¸€ä¸ªå®ä¾‹è¿è¡Œï¼ˆé€šè¿‡é”æ–‡ä»¶ï¼‰"""
    import sys
    import os
    
    lock_file = Path(__file__).parent / ".bot.lock"
    
    # æ£€æŸ¥é”æ–‡ä»¶
    if lock_file.exists():
        try:
            with open(lock_file, "r") as f:
                old_pid = int(f.read().strip())
            
            # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
            import subprocess
            result = subprocess.run(
                ["tasklist", "/FI", f"PID eq {old_pid}"],
                capture_output=True, text=True
            )
            if str(old_pid) in result.stdout:
                print(f"[ERROR] æœºå™¨äººå·²åœ¨è¿è¡Œä¸­ (PID: {old_pid})")
                print("å¦‚éœ€é‡å¯ï¼Œè¯·å…ˆå…³é—­ç°æœ‰è¿›ç¨‹")
                sys.exit(1)
        except (ValueError, FileNotFoundError):
            pass  # é”æ–‡ä»¶æŸåæˆ–è¿›ç¨‹å·²ä¸å­˜åœ¨ï¼Œå¯ä»¥ç»§ç»­
    
    # å†™å…¥å½“å‰PID
    with open(lock_file, "w") as f:
        f.write(str(os.getpid()))
    
    # æ³¨å†Œé€€å‡ºæ—¶æ¸…ç†é”æ–‡ä»¶
    import atexit
    def cleanup():
        try:
            lock_file.unlink()
        except:
            pass
    atexit.register(cleanup)
    
    return True


def main():
    global KB_DOCUMENTS

    # ç¡®ä¿å•å®ä¾‹è¿è¡Œ
    check_single_instance()
    
    load_config()
    KB_DOCUMENTS = load_knowledge_base()

    print("=" * 50)
    print("æ–¯å¦æ˜ŸçƒçŸ¥è¯†åº“é’‰é’‰æœºå™¨äºº (Streamæ¨¡å¼)")
    print(f"å·²åŠ è½½ {len(KB_DOCUMENTS)} ä¸ªæ–‡æ¡£")
    print(f"è¿›ç¨‹PID: {__import__('os').getpid()}")
    print("=" * 50)
    print("\næ”¯æŒç§èŠå’Œç¾¤èŠ")
    print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡\n")

    # åˆ›å»ºStreamå®¢æˆ·ç«¯
    credential = dingtalk_stream.Credential(
        CONFIG["app_key"],
        CONFIG["app_secret"]
    )
    client = dingtalk_stream.DingTalkStreamClient(credential)

    # æ³¨å†Œæ¶ˆæ¯å¤„ç†å™¨
    client.register_callback_handler(
        dingtalk_stream.chatbot.ChatbotMessage.TOPIC,
        StarplanetKnowledgeHandler()
    )

    # å¯åŠ¨
    client.start_forever()


if __name__ == "__main__":
    main()
